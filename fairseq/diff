9d8
< import os.path as op
11c10,12
< from typing import Dict, List, Optional, Tuple
---
> from collections import defaultdict
> from pathlib import Path
> from typing import Dict, List, Optional, NamedTuple
22c23,31
< from fairseq.data.audio.audio_utils import get_fbank, get_waveform
---
> from fairseq.data.audio.audio_utils import (
>     get_fbank,
>     get_waveform,
>     read_from_stored_zip,
>     is_npy_data,
>     is_sf_audio_data,
>     parse_path,
>     FEATURE_OR_SF_AUDIO_FILE_EXTENSIONS,
> )
32c41
<     def __init__(self, yaml_path):
---
>     def __init__(self, yaml_path: Path):
36c45
<             print("Please install PyYAML to load YAML files for " "S2T data config")
---
>             print("Please install PyYAML to load YAML files for S2T data config")
38c47
<         if op.isfile(yaml_path):
---
>         if yaml_path.is_file():
42,43c51,54
<             except:
<                 raise Exception(f"Failed to load config from {yaml_path}: {e}")
---
>             except Exception as e:
>                 raise Exception(
>                     f"Failed to load config from {yaml_path.as_posix()}: {e}"
>                 )
45,46c56
<             raise FileNotFoundError(f"{yaml_path} not found")
< 
---
>             raise FileNotFoundError(f"{yaml_path.as_posix()} not found")
103a114,119
>     def use_sample_rate(self):
>         """Needed by the dataset loader to see if the model requires
>         raw audio with specific sample rate as inputs."""
>         return self.config.get("use_sample_rate", 16000)
> 
>     @property
124,140d139
< def is_npy_data(data: bytes) -> bool:
<     return data[0] == 147 and data[1] == 78
< 
< 
< def is_flac_or_wav_data(data: bytes) -> bool:
<     is_flac = data[0] == 102 and data[1] == 76
<     is_wav = data[0] == 82 and data[1] == 73
<     return is_flac or is_wav
< 
< 
< def read_from_uncompressed_zip(file_path, offset, file_size) -> bytes:
<     with open(file_path, "rb") as f:
<         f.seek(offset)
<         data = f.read(file_size)
<     return data
< 
< 
142,143c141,142
<     ext = op.splitext(op.basename(path))[1]
<     if ext not in {".npy", ".flac", ".wav"}:
---
>     ext = Path(path).suffix
>     if ext not in FEATURE_OR_SF_AUDIO_FILE_EXTENSIONS:
148,149c147,152
< def get_features_or_waveform_from_uncompressed_zip(
<     path, byte_offset, byte_size, need_waveform=False
---
> def get_features_or_waveform_from_stored_zip(
>     path,
>     byte_offset,
>     byte_size,
>     need_waveform=False,
>     use_sample_rate=-1,
152c155
<     data = read_from_uncompressed_zip(path, byte_offset, byte_size)
---
>     data = read_from_stored_zip(path, byte_offset, byte_size)
156,157c159,164
<     elif is_flac_or_wav_data(data):
<         features_or_waveform = get_waveform(f)[0] if need_waveform else get_fbank(f)
---
>     elif is_sf_audio_data(data):
>         features_or_waveform = (
>             get_waveform(f, always_2d=False, output_sample_rate=use_sample_rate)[0]
>             if need_waveform
>             else get_fbank(f)
>         )
163c170
< def get_features_or_waveform(path: str, need_waveform=False):
---
> def get_features_or_waveform(path: str, need_waveform=False, use_sample_rate=-1):
171a179
>         use_sample_rate (int): change sample rate for the input wave file
176,180c184,185
<     _path, *extra = path.split(":")
<     if not op.exists(_path):
<         raise FileNotFoundError(f"File not found: {_path}")
< 
<     if len(extra) == 0:
---
>     _path, slice_ptr = parse_path(path)
>     if len(slice_ptr) == 0:
182c187,189
<             return get_waveform(_path)
---
>             return get_waveform(
>                 _path, always_2d=False, output_sample_rate=use_sample_rate
>             )[0]
184,187c191,197
<     elif len(extra) == 2:
<         extra = [int(i) for i in extra]
<         features_or_waveform = get_features_or_waveform_from_uncompressed_zip(
<             _path, extra[0], extra[1], need_waveform=need_waveform
---
>     elif len(slice_ptr) == 2:
>         features_or_waveform = get_features_or_waveform_from_stored_zip(
>             _path,
>             slice_ptr[0],
>             slice_ptr[1],
>             need_waveform=need_waveform,
>             use_sample_rate=use_sample_rate,
215a226,231
> class SpeechToTextDatasetItem(NamedTuple):
>     index: int
>     source: torch.Tensor
>     target: Optional[torch.Tensor] = None
> 
> 
223c239
<         data_cfg: S2TDataConfig,
---
>         cfg: S2TDataConfig,
228d243
<         pred_texts: Optional[List[str]] = None,
238c253
<         self.data_cfg = data_cfg
---
>         self.cfg = cfg
252,255d266
<         if None not in pred_texts:
<             self.pred_texts = pred_texts
<         else:
<             self.pred_texts = None
260c271
<         self.shuffle = data_cfg.shuffle if is_train_split else False
---
>         self.shuffle = cfg.shuffle if is_train_split else False
263c274
<             self.data_cfg.get_feature_transforms(split, is_train_split)
---
>             self.cfg.get_feature_transforms(split, is_train_split)
268a280,281
>         self.tgt_lens = self.get_tgt_lens_and_check_oov()
> 
270a284,301
>     def get_tgt_lens_and_check_oov(self):
>         if self.tgt_texts is None:
>             return [0 for _ in range(self.n_samples)]
>         tgt_lens = []
>         n_tokens, n_oov_tokens = 0, 0
>         for i in range(self.n_samples):
>             tokenized = self.get_tokenized_tgt_text(i).split(" ")
>             oov_tokens = [
>                 t
>                 for t in tokenized
>                 if self.tgt_dict.index(t) == self.tgt_dict.unk_index
>             ]
>             n_tokens += len(tokenized)
>             n_oov_tokens += len(oov_tokens)
>             tgt_lens.append(len(tokenized))
>         logger.info(f"'{self.split}' has {n_oov_tokens / n_tokens * 100:.2f}% OOV")
>         return tgt_lens
> 
275c306
<             f"prepend_tgt_lang_tag={self.data_cfg.prepend_tgt_lang_tag}, "
---
>             f"prepend_tgt_lang_tag={self.cfg.prepend_tgt_lang_tag}, "
285c316
<         if self.data_cfg.prepend_tgt_lang_tag:
---
>         if self.cfg.prepend_tgt_lang_tag:
292,296c323,329
<     def tokenize_text(self, text: str):
<         if self.pre_tokenizer is not None:
<             text = self.pre_tokenizer.encode(text)
<         if self.bpe_tokenizer is not None:
<             text = self.bpe_tokenizer.encode(text)
---
>     @classmethod
>     def tokenize(cls, tokenizer, text: str):
>         return text if tokenizer is None else tokenizer.encode(text)
> 
>     def get_tokenized_tgt_text(self, index: int):
>         text = self.tokenize(self.pre_tokenizer, self.tgt_texts[index])
>         text = self.tokenize(self.bpe_tokenizer, text)
299,301c332,338
<     def __getitem__(
<         self, index: int
<     ) -> Tuple[int, torch.Tensor, Optional[torch.Tensor]]:
---
>     @classmethod
>     def get_lang_tag_idx(cls, lang: str, dictionary: Dictionary):
>         lang_tag_idx = dictionary.index(cls.LANG_TAG_TEMPLATE.format(lang))
>         assert lang_tag_idx != dictionary.unk()
>         return lang_tag_idx
> 
>     def __getitem__(self, index: int) -> SpeechToTextDatasetItem:
303c340,342
<             self.audio_paths[index], need_waveform=self.data_cfg.use_audio_input
---
>             self.audio_paths[index],
>             need_waveform=self.cfg.use_audio_input,
>             use_sample_rate=self.cfg.use_sample_rate,
306c345
<             assert not self.data_cfg.use_audio_input
---
>             assert not self.cfg.use_audio_input
308a348
> 
310d349
<         pred = None
312c351
<             tokenized = self.tokenize_text(self.tgt_texts[index])
---
>             tokenized = self.get_tokenized_tgt_text(index)
316,318c355,358
<             if self.data_cfg.prepend_tgt_lang_tag:
<                 lang_tag = self.LANG_TAG_TEMPLATE.format(self.tgt_langs[index])
<                 lang_tag_idx = self.tgt_dict.index(lang_tag)
---
>             if self.cfg.prepend_tgt_lang_tag:
>                 lang_tag_idx = self.get_lang_tag_idx(
>                     self.tgt_langs[index], self.tgt_dict
>                 )
320,330c360,361
<         if self.pred_texts is not None:
<             tokenized = self.tokenize_text(self.pred_texts[index])
<             pred = self.tgt_dict.encode_line(
<                 tokenized, add_if_not_exist=False, append_eos=True
<             ).long()
<             if self.data_cfg.prepend_tgt_lang_tag:
<                 lang_tag = self.LANG_TAG_TEMPLATE.format(self.tgt_langs[index])
<                 lang_tag_idx = self.tgt_dict.index(lang_tag)
<                 pred = torch.cat((torch.LongTensor([lang_tag_idx]), pred), 0)
<             return index, source, target, pred
<         return index, source, target
---
> 
>         return SpeechToTextDatasetItem(index=index, source=source, target=target)
335c366,368
<     def collater(self, samples: List[Tuple[int, torch.Tensor, torch.Tensor]]) -> Dict:
---
>     def collater(
>         self, samples: List[SpeechToTextDatasetItem], return_order: bool = False
>     ) -> Dict:
338,363c371,382
<         if self.pred_texts is not None:
<             indices = torch.tensor([i for i, _, _, _ in samples], dtype=torch.long)
<             frames = _collate_frames(
<                 [s for _, s, _, _ in samples], self.data_cfg.use_audio_input
<             )
<             n_frames = torch.tensor([s.size(0) for _, s, _, _ in samples], dtype=torch.long)
<             # sort samples by descending number of frames
<             n_frames, order = n_frames.sort(descending=True)
<             indices = indices.index_select(0, order)
<             frames = frames.index_select(0, order)
< 
<             target, target_lengths = None, None
<             prev_output_tokens = None
<             ntokens = None
<             pred = fairseq_data_utils.collate_tokens(
<                 [t for _, _, _, t in samples],
<                 self.tgt_dict.pad(),
<                 self.tgt_dict.eos(),
<                 left_pad=False,
<                 move_eos_to_beginning=False,
<             )
<             pred = pred.index_select(0, order)
<             indices = torch.tensor([i for i, _, _, _ in samples], dtype=torch.long)
<             frames = _collate_frames(
<                 [s for _, s, _, _ in samples], self.data_cfg.use_audio_input
<             )
---
>         indices = torch.tensor([x.index for x in samples], dtype=torch.long)
>         frames = _collate_frames([x.source for x in samples], self.cfg.use_audio_input)
>         # sort samples by descending number of frames
>         n_frames = torch.tensor([x.source.size()[0] for x in samples], dtype=torch.long)
>         n_frames, order = n_frames.sort(descending=True)
>         indices = indices.index_select(0, order)
>         frames = frames.index_select(0, order)
> 
>         target, target_lengths = None, None
>         prev_output_tokens = None
>         ntokens = None
>         if self.tgt_texts is not None:
365c384
<                 [t for _, _, t, _ in samples],
---
>                 [x.target for x in samples],
373c392
<                 [t.size(0) for _, _, t, _ in samples], dtype=torch.long
---
>                 [x.target.size()[0] for x in samples], dtype=torch.long
376c395
<                 [t for _, _, t, _ in samples],
---
>                 [x.target for x in samples],
383,433c402
<             ntokens = sum(t.size(0) for _, _, t, _ in samples)
<             out = {
<                 "id": indices,
<                 "net_input": {
<                     "src_tokens": frames,
<                     "src_lengths": n_frames,
<                     "prev_output_tokens": prev_output_tokens,
<                 },
<                 "target": target,
<                 "target_lengths": target_lengths,
<                 "ntokens": ntokens,
<                 "nsentences": len(samples),
<                 "prediction": pred
<             }
< 
<         else:
<             indices = torch.tensor([i for i, _, _ in samples], dtype=torch.long)
<             frames = _collate_frames(
<                 [s for _, s, _ in samples], self.data_cfg.use_audio_input
<             )
<             n_frames = torch.tensor([s.size(0) for _, s, _ in samples], dtype=torch.long)
<             # sort samples by descending number of frames
<             n_frames, order = n_frames.sort(descending=True)
<             indices = indices.index_select(0, order)
<             frames = frames.index_select(0, order)
< 
<             target, target_lengths = None, None
<             prev_output_tokens = None
<             ntokens = None
<  
<             if self.tgt_texts is not None:
<                 target = fairseq_data_utils.collate_tokens(
<                     [t for _, _, t in samples],
<                     self.tgt_dict.pad(),
<                     self.tgt_dict.eos(),
<                     left_pad=False,
<                     move_eos_to_beginning=False,
<                 )
<                 target = target.index_select(0, order)
<                 target_lengths = torch.tensor(
<                     [t.size(0) for _, _, t in samples], dtype=torch.long
<                 ).index_select(0, order)
<                 prev_output_tokens = fairseq_data_utils.collate_tokens(
<                     [t for _, _, t in samples],
<                     self.tgt_dict.pad(),
<                     self.tgt_dict.eos(),
<                     left_pad=False,
<                     move_eos_to_beginning=True,
<                 )
<                 prev_output_tokens = prev_output_tokens.index_select(0, order)
<                 ntokens = sum(t.size(0) for _, _, t in samples)
---
>             ntokens = sum(x.target.size()[0] for x in samples)
435,446c404,418
<                 out = {
<                 "id": indices,
<                 "net_input": {
<                     "src_tokens": frames,
<                     "src_lengths": n_frames,
<                     "prev_output_tokens": prev_output_tokens,
<                 },
<                 "target": target,
<                 "target_lengths": target_lengths,
<                 "ntokens": ntokens,
<                 "nsentences": len(samples),
<             }
---
>         net_input = {
>             "src_tokens": frames,
>             "src_lengths": n_frames,
>             "prev_output_tokens": prev_output_tokens,
>         }
>         out = {
>             "id": indices,
>             "net_input": net_input,
>             "target": target,
>             "target_lengths": target_lengths,
>             "ntokens": ntokens,
>             "nsentences": len(samples),
>         }
>         if return_order:
>             out["order"] = order
453,457c425
<         t_len = 0
<         if self.tgt_texts is not None:
<             tokenized = self.tokenize_text(self.tgt_texts[index])
<             t_len = len(tokenized.split(" "))
<         return self.n_frames[index], t_len
---
>         return self.n_frames[index], self.tgt_lens[index]
495,496c463,464
<         samples: List[List[Dict]],
<         data_cfg: S2TDataConfig,
---
>         samples: List[Dict],
>         cfg: S2TDataConfig,
501,517c469,477
<         audio_paths, n_frames, src_texts, tgt_texts, ids = [], [], [], [], []
<         pred_texts = []
<         speakers, src_langs, tgt_langs = [], [], []
<         for s in samples:
<             ids.extend([ss[cls.KEY_ID] for ss in s])
<             audio_paths.extend(
<                 [op.join(data_cfg.audio_root, ss[cls.KEY_AUDIO]) for ss in s]
<             )
<             n_frames.extend([int(ss[cls.KEY_N_FRAMES]) for ss in s])
<             tgt_texts.extend([ss[cls.KEY_TGT_TEXT] for ss in s])
<             src_texts.extend(
<                 [ss.get(cls.KEY_SRC_TEXT, cls.DEFAULT_SRC_TEXT) for ss in s]
<             )
<             speakers.extend([ss.get(cls.KEY_SPEAKER, cls.DEFAULT_SPEAKER) for ss in s])
<             src_langs.extend([ss.get(cls.KEY_SRC_LANG, cls.DEFAULT_LANG) for ss in s])
<             tgt_langs.extend([ss.get(cls.KEY_TGT_LANG, cls.DEFAULT_LANG) for ss in s])
<             pred_texts.extend([ss.get("predictions", None) for ss in s])
---
>         audio_root = Path(cfg.audio_root)
>         ids = [s[cls.KEY_ID] for s in samples]
>         audio_paths = [(audio_root / s[cls.KEY_AUDIO]).as_posix() for s in samples]
>         n_frames = [int(s[cls.KEY_N_FRAMES]) for s in samples]
>         tgt_texts = [s[cls.KEY_TGT_TEXT] for s in samples]
>         src_texts = [s.get(cls.KEY_SRC_TEXT, cls.DEFAULT_SRC_TEXT) for s in samples]
>         speakers = [s.get(cls.KEY_SPEAKER, cls.DEFAULT_SPEAKER) for s in samples]
>         src_langs = [s.get(cls.KEY_SRC_LANG, cls.DEFAULT_LANG) for s in samples]
>         tgt_langs = [s.get(cls.KEY_TGT_LANG, cls.DEFAULT_LANG) for s in samples]
521c481
<             data_cfg,
---
>             cfg,
524,533c484,492
<             src_texts,
<             tgt_texts,
<             pred_texts,
<             speakers,
<             src_langs,
<             tgt_langs,
<             ids,
<             tgt_dict,
<             pre_tokenizer,
<             bpe_tokenizer,
---
>             src_texts=src_texts,
>             tgt_texts=tgt_texts,
>             speakers=speakers,
>             src_langs=src_langs,
>             tgt_langs=tgt_langs,
>             ids=ids,
>             tgt_dict=tgt_dict,
>             pre_tokenizer=pre_tokenizer,
>             bpe_tokenizer=bpe_tokenizer,
537c496,498
<     def _get_size_ratios(cls, ids: List[str], sizes: List[int], alpha: float = 1.0):
---
>     def get_size_ratios(
>         cls, datasets: List[SpeechToTextDataset], alpha: float = 1.0
>     ) -> List[float]:
540,552c501,561
<         _sizes = np.array(sizes)
<         prob = _sizes / _sizes.sum()
<         smoothed_prob = prob ** alpha
<         smoothed_prob = smoothed_prob / smoothed_prob.sum()
<         size_ratio = (smoothed_prob * _sizes.sum()) / _sizes
< 
<         o_str = str({_i: f"{prob[i]:.3f}" for i, _i in enumerate(ids)})
<         logger.info(f"original sampling probability: {o_str}")
<         p_str = str({_i: f"{smoothed_prob[i]:.3f}" for i, _i in enumerate(ids)})
<         logger.info(f"balanced sampling probability: {p_str}")
<         sr_str = str({_id: f"{size_ratio[i]:.3f}" for i, _id in enumerate(ids)})
<         logger.info(f"balanced sampling size ratio: {sr_str}")
<         return size_ratio.tolist()
---
> 
>         id_to_lp, lp_to_sz = {}, defaultdict(int)
>         for ds in datasets:
>             lang_pairs = {f"{s}->{t}" for s, t in zip(ds.src_langs, ds.tgt_langs)}
>             assert len(lang_pairs) == 1
>             lang_pair = list(lang_pairs)[0]
>             id_to_lp[ds.split] = lang_pair
>             lp_to_sz[lang_pair] += sum(ds.n_frames)
> 
>         sz_sum = sum(v for v in lp_to_sz.values())
>         lp_to_prob = {k: v / sz_sum for k, v in lp_to_sz.items()}
>         lp_to_tgt_prob = {k: v ** alpha for k, v in lp_to_prob.items()}
>         prob_sum = sum(v for v in lp_to_tgt_prob.values())
>         lp_to_tgt_prob = {k: v / prob_sum for k, v in lp_to_tgt_prob.items()}
>         lp_to_sz_ratio = {
>             k: (lp_to_tgt_prob[k] * sz_sum) / v for k, v in lp_to_sz.items()
>         }
>         size_ratio = [lp_to_sz_ratio[id_to_lp[ds.split]] for ds in datasets]
> 
>         p_formatted = {
>             k: f"{lp_to_prob[k]:.3f}->{lp_to_tgt_prob[k]:.3f}" for k in lp_to_sz
>         }
>         logger.info(f"sampling probability balancing: {p_formatted}")
>         sr_formatted = {ds.split: f"{r:.3f}" for ds, r in zip(datasets, size_ratio)}
>         logger.info(f"balanced sampling size ratio: {sr_formatted}")
>         return size_ratio
> 
>     @classmethod
>     def _load_samples_from_tsv(cls, root: str, split: str):
>         tsv_path = Path(root) / f"{split}.tsv"
>         if not tsv_path.is_file():
>             raise FileNotFoundError(f"Dataset not found: {tsv_path}")
>         with open(tsv_path) as f:
>             reader = csv.DictReader(
>                 f,
>                 delimiter="\t",
>                 quotechar=None,
>                 doublequote=False,
>                 lineterminator="\n",
>                 quoting=csv.QUOTE_NONE,
>             )
>             samples = [dict(e) for e in reader]
>         if len(samples) == 0:
>             raise ValueError(f"Empty manifest: {tsv_path}")
>         return samples
> 
>     @classmethod
>     def _from_tsv(
>         cls,
>         root: str,
>         cfg: S2TDataConfig,
>         split: str,
>         tgt_dict,
>         is_train_split: bool,
>         pre_tokenizer,
>         bpe_tokenizer,
>     ) -> SpeechToTextDataset:
>         samples = cls._load_samples_from_tsv(root, split)
>         return cls._from_list(
>             split, is_train_split, samples, cfg, tgt_dict, pre_tokenizer, bpe_tokenizer
>         )
558c567
<         data_cfg: S2TDataConfig,
---
>         cfg: S2TDataConfig,
567,597d575
<         samples = []
<         _splits = splits.split(",")
<         for split in _splits:
<             tsv_path = op.join(root, f"{split}.tsv")
<             if not op.isfile(tsv_path):
<                 raise FileNotFoundError(f"Dataset not found: {tsv_path}")
<             with open(tsv_path) as f:
<                 reader = csv.DictReader(
<                     f,
<                     delimiter="\t",
<                     quotechar=None,
<                     doublequote=False,
<                     lineterminator="\n",
<                     quoting=csv.QUOTE_NONE,
<                 )
<                 sample = []
<                 preds = False
<                 for e in reader:
<                     e = dict(e)
<                     if "predictions" in e.keys():
<                         preds = True
<                         # matching pred -> target didn't work
<                         if e["predictions"] == 0 or e["predictions"] == None:
<                             continue
<                     else:
<                         if preds:
<                             continue
<                     sample.append(e)
<                 samples.append(sample)
<                 assert len(samples) > 0
< 
599,606c577,578
<             cls._from_list(
<                 name,
<                 is_train_split,
<                 [s],
<                 data_cfg,
<                 tgt_dict,
<                 pre_tokenizer,
<                 bpe_tokenizer,
---
>             cls._from_tsv(
>                 root, cfg, split, tgt_dict, is_train_split, pre_tokenizer, bpe_tokenizer
608c580
<             for name, s in zip(_splits, samples)
---
>             for split in splits.split(",")
611c583
<         if is_train_split and len(_splits) > 1 and data_cfg.sampling_alpha != 1.0:
---
>         if is_train_split and len(datasets) > 1 and cfg.sampling_alpha != 1.0:
613,615c585
<             size_ratios = cls._get_size_ratios(
<                 _splits, [len(s) for s in samples], alpha=data_cfg.sampling_alpha
<             )
---
>             size_ratios = cls.get_size_ratios(datasets, alpha=cfg.sampling_alpha)
620c590
<                 for d, r in zip(datasets, size_ratios)
---
>                 for r, d in zip(size_ratios, datasets)
622c592,593
<         return ConcatDataset(datasets)
---
> 
>         return ConcatDataset(datasets) if len(datasets) > 1 else datasets[0]
